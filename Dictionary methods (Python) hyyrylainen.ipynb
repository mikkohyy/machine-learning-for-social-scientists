{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary methods\n",
    "\n",
    "## Dataset for the exercise\n",
    "\n",
    "* [New York Times Comments](https://www.kaggle.com/aashita/nyt-comments/data), set of reders' comments for articles published in the New York Times\n",
    "\n",
    "## Overarching research question\n",
    "\n",
    "The comments allow a perspective to study what kind of concerns people raise when commenting to online articles.\n",
    "Study what seem to be the target of the commenting: New York Times staff or journalistic guidelines (suggesting that comments serveas a tool for journalists to interact with their audiences _or_ other audience members)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The logic of having three set of keywords is that since we are interested about New York Times' staff and journalistic \n",
    "## guidelines, first thing we need find out what comments are about New York Times itself and after that we need to\n",
    "## understand the category the comment belongs to. \n",
    "\n",
    "keywords = \"New York Times, NYT ,staff,journalist,writer,editor,ethics,truth,fairness,diversity\"\n",
    "keywords = keywords.lower()\n",
    "keywords = keywords.split(',')\n",
    "\n",
    "keywords_nyt = \"New York Times, NYT \"\n",
    "keywords_nyt = keywords_nyt.lower()\n",
    "keywords_nyt = keywords_nyt.split(',')\n",
    "\n",
    "keywords_staff = 'staff,journalist,writer,editor'\n",
    "keywords_staff = keywords_staff.lower()\n",
    "keywords_staff = keywords_staff.split(',')\n",
    "\n",
    "keywords_ethics = 'ethics,truth,fairness,diversity,honesty'\n",
    "keywords_ethics = keywords_ethics.lower()\n",
    "keywords_ethics = keywords_ethics.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/nyt/'\n",
    "files = os.listdir( path ) ## see all files in directory\n",
    "files = filter( lambda file_name: file_name.startswith(\"Comments\"), files ) ## choose only data files\n",
    "files = map( lambda file_name: path + file_name, files ) ## add path to file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156957 comments mention any of these: new york times, nyt ,staff,journalist,writer,editor,ethics,truth,fairness,diversity\n",
      "There are in total 2176364\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "comments = 0\n",
    "\n",
    "for file in files:\n",
    "    for entry in csv.DictReader( open( file, encoding='utf8') ):\n",
    "        \n",
    "        comments += 1\n",
    "        \n",
    "        comment = entry['commentBody']\n",
    "        \n",
    "        ## work through several different keywords in the analysis\n",
    "            \n",
    "        for keyword in keywords:\n",
    "            if keyword in comment.lower():\n",
    "                counter += 1\n",
    "                break\n",
    "   \n",
    "print( counter, \"comments mention any of these:\", ','.join(keywords) )\n",
    "print( \"There are in total\", comments )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Identify other potential keywords for this phenomena and add those keywords in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created three sets of keywords. First one to find the articles that are about New York Times. Second one to find the articles that are about the staff of NYT. Third one to find the articles that are about the journalistic guidelines of NYT.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Are there any cases where this approach might break? modify the code to mitigate them when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the whitespaces mattered. Meaning that if the keyword is \"nyt\", it matches with words that have \"nyt\" in their body. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data has `createDate` variable as well which identifies when the comment was created. Based on this, try to look for some temporal trends in comment counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements the new keyword \"strategy\" and also maps the number of comments in each month: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/nyt/'\n",
    "files = os.listdir( path ) ## see all files in directory\n",
    "files = filter( lambda file_name: file_name.startswith(\"Comments\"), files ) ## choose only data files\n",
    "files = map( lambda file_name: path + file_name, files ) ## add path to file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35694 comments mention any of these: new york times, nyt \n",
      "5867 comments mention any of these: staff,journalist,writer,editor\n",
      "2625 comments mention any of these: ethics,truth,fairness,diversity,honesty\n",
      "599 comments that mention staff and ethics keywords\n",
      "There are in total 2176364 \n",
      "\n",
      "About nyt:  OrderedDict([('Jan 2017', 9900), ('Feb 2017', 8378), ('Mar 2017', 8824), ('Apr 2017', 7796), ('May 2017', 10258), ('Jan 2018', 5774), ('Feb 2018', 5602), ('Mar 2018', 6948), ('Apr 2018', 6796)]) \n",
      "\n",
      "About staff:  OrderedDict([('Jan 2017', 887), ('Feb 2017', 714), ('Mar 2017', 666), ('Apr 2017', 674), ('May 2017', 1041), ('Jan 2018', 434), ('Feb 2018', 406), ('Mar 2018', 511), ('Apr 2018', 534)]) \n",
      "\n",
      "About ethics: OrderedDict([('Jan 2017', 474), ('Feb 2017', 329), ('Mar 2017', 354), ('Apr 2017', 278), ('May 2017', 373), ('Jan 2018', 227), ('Feb 2018', 187), ('Mar 2018', 182), ('Apr 2018', 221)]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments = 0\n",
    "\n",
    "comments_about_nyt_counter = 0\n",
    "comments_about_staff_counter = 0\n",
    "comments_about_ethics_counter = 0\n",
    "comments_in_both_categories = 0\n",
    "\n",
    "wanted_months = ['Jan 2017', 'Feb 2017', 'Mar 2017', 'Apr 2017', 'May 2017', 'Jan 2018', 'Feb 2018', 'Mar 2018', \n",
    "                'Apr 2018'] # In the 2017 data there were also comments from June to December\n",
    "\n",
    "nyt_comments_per_month = OrderedDict([('Jan 2017', 0), ('Feb 2017', 0), ('Mar 2017', 0), ('Apr 2017', 0), \n",
    "                                      ('May 2017', 0), ('Jan 2018', 0), ('Feb 2018', 0), ('Mar 2018', 0), \n",
    "                                      ('Apr 2018', 0)])\n",
    "\n",
    "staff_comments_per_month = OrderedDict([('Jan 2017', 0), ('Feb 2017', 0), ('Mar 2017', 0), ('Apr 2017', 0), \n",
    "                                      ('May 2017', 0), ('Jan 2018', 0), ('Feb 2018', 0), ('Mar 2018', 0), \n",
    "                                      ('Apr 2018', 0)])\n",
    "\n",
    "ethics_comments_per_month = OrderedDict([('Jan 2017', 0), ('Feb 2017', 0), ('Mar 2017', 0), ('Apr 2017', 0), \n",
    "                                      ('May 2017', 0), ('Jan 2018', 0), ('Feb 2018', 0), ('Mar 2018', 0), \n",
    "                                      ('Apr 2018', 0)])\n",
    "\n",
    "for file in files:\n",
    "    for entry in csv.DictReader( open( file, encoding='utf-8') ):\n",
    "        \n",
    "        comments += 1\n",
    "        \n",
    "        comment = entry['commentBody']\n",
    "        create_date = entry['createDate']\n",
    "        \n",
    "        comment_time = float(create_date)        \n",
    "        \n",
    "        ## First, lets filter the comments that mention keywords new york times or nyt. So we have the body of\n",
    "        ## comments that are about the magazine itself:\n",
    "        for keyword_nyt in keywords_nyt:\n",
    "            \n",
    "                       \n",
    "            was_in_staff = False\n",
    "            \n",
    "            if keyword_nyt in comment.lower():\n",
    "                comments_about_nyt_counter += 1\n",
    "                \n",
    "                month_of_comment = datetime.datetime.fromtimestamp(int(comment_time)).strftime('%b %Y')\n",
    "                \n",
    "                if month_of_comment in wanted_months:\n",
    "                \n",
    "                    nyt_comments_per_month[month_of_comment] = nyt_comments_per_month[month_of_comment] + 1\n",
    "\n",
    "                    for keyword_staff in keywords_staff:\n",
    "                        if keyword_staff in comment.lower():\n",
    "                            comments_about_staff_counter += 1    \n",
    "                            was_in_staff = True\n",
    "                            staff_comments_per_month[month_of_comment] = staff_comments_per_month[month_of_comment] + 1\n",
    "                            break\n",
    "\n",
    "                    for keyword_ethics in keywords_ethics:\n",
    "                        if keyword_ethics in comment.lower():\n",
    "                            comments_about_ethics_counter += 1\n",
    "                            ethics_comments_per_month[month_of_comment] = ethics_comments_per_month[month_of_comment] + 1\n",
    "                            if was_in_staff == True:\n",
    "                                comments_in_both_categories += 1\n",
    "                            break\n",
    "\n",
    "                    break\n",
    "                \n",
    "print( comments_about_nyt_counter, \"comments mention any of these:\", ','.join(keywords_nyt) )\n",
    "print( comments_about_staff_counter, \"comments mention any of these:\", ','.join(keywords_staff) )\n",
    "print( comments_about_ethics_counter, \"comments mention any of these:\", ','.join(keywords_ethics) )\n",
    "print( comments_in_both_categories, \"comments that mention staff and ethics keywords\" )\n",
    "print( \"There are in total\", comments, '\\n' )\n",
    "\n",
    "print('About nyt: ', nyt_comments_per_month, '\\n')\n",
    "print('About staff: ', staff_comments_per_month, '\\n')\n",
    "print('About ethics:', ethics_comments_per_month, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, defining the right keywords seems to be very difficult task. I am not sure if first filtering those comments that are about new york times and then checking in which category they fall is reasonable since it only takes only 35 685 comments from 2 176 364 possible comments. If we look for some kind of temporal trends we can say that the there is probably something wrong with the data (or my code) with comments of April 2017 since compared to the other months there are very few comments. However, what I find interesting is that January 2017 and May 2018 seem to have been quite busy time for commenting. In 2018 it seems that the March and April are the busiest times for commenting. There does not seem to be any noteworthy trends in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language analysis\n",
    "\n",
    "In many languages, different words can have different forms. For example, 'I have an apple' and 'I have several apples' convey almost the same information, similarly 'She had an apple' and 'She has an apple' are almost identical. In Finnish language, such examples are much more extensive thanks to the many suffixes words may have several forms.\n",
    "\n",
    "![Joke about conjugation](https://i1.wp.com/finnishteacher.com/wp-content/uploads/2018/11/finnish-language-meme.png?resize=1024%2C591&ssl=1)\n",
    "\n",
    "This might make analysis difficult! Therefore often the language is **stemmed** or **lemmatized** into its basic form. Furthermore, tools such as [Natural Language Toolkit](https://www.nltk.org/) allow parsing text to identify proper nouns, identify named entities or determine if a word is adjective, noun etc.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Use same dataset.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "Replicate the previous exercise using proper stemmatization. If results change, how and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a longer exampl ! mani word are includ here , and we shall all word . \n"
     ]
    }
   ],
   "source": [
    "message = 'This is a longer example! Many words are included here, and we shall all words.'\n",
    "stemmed = ''\n",
    "\n",
    "for word in nltk.word_tokenize( message ):\n",
    "    stemmed += stemmer.stem( word ) + ' '\n",
    "    \n",
    "print( stemmed )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming the keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_nyt = \"New York Times, NYT \"\n",
    "\n",
    "stemmed = ''\n",
    "for word in nltk.word_tokenize( keywords_nyt ):\n",
    "    stemmed += stemmer.stem( word ) + ' '\n",
    "\n",
    "keywords_nyt = stemmed.lower()\n",
    "keywords_nyt = keywords_nyt.split(',')\n",
    "\n",
    "keywords_staff = 'staff,journalist,writer,editor'\n",
    "\n",
    "stemmed = ''\n",
    "for word in nltk.word_tokenize( keywords_staff ):\n",
    "    stemmed += stemmer.stem( word ) + ' '\n",
    "\n",
    "keywords_staff = stemmed.lower()\n",
    "keywords_staff = keywords_staff.split(',')\n",
    "\n",
    "keywords_ethics = 'ethics,truth,fairness,diversity,honesty'\n",
    "\n",
    "stemmed = ''\n",
    "for word in nltk.word_tokenize( keywords_ethics ):\n",
    "    stemmed += stemmer.stem( word ) + ' '\n",
    "\n",
    "keywords_ethics = stemmed.lower()\n",
    "keywords_ethics = keywords_ethics.split(',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/nyt/'\n",
    "files = os.listdir( path ) ## see all files in directory\n",
    "files = filter( lambda file_name: file_name.startswith(\"Comments\"), files ) ## choose only data files\n",
    "files = map( lambda file_name: path + file_name, files ) ## add path to file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25203 comments mention any of these: new york time , nyt \n",
      "2515 comments mention any of these: staff , journalist , writer , editor \n",
      "2413 comments mention any of these: ethic , truth , fair , divers , honesti \n",
      "352 comments that mention staff and ethics keywords\n",
      "There are in total 2176364 comments\n",
      "\n",
      "About nyt:  OrderedDict([('Jan 2017', 3568), ('Feb 2017', 2956), ('Mar 2017', 3134), ('Apr 2017', 2765), ('May 2017', 3665), ('Jan 2018', 2052), ('Feb 2018', 2060), ('Mar 2018', 2555), ('Apr 2018', 2448)]) \n",
      "\n",
      "About staff:  OrderedDict([('Jan 2017', 394), ('Feb 2017', 315), ('Mar 2017', 288), ('Apr 2017', 301), ('May 2017', 398), ('Jan 2018', 179), ('Feb 2018', 184), ('Mar 2018', 228), ('Apr 2018', 228)]) \n",
      "\n",
      "About ethics: OrderedDict([('Jan 2017', 398), ('Feb 2017', 263), ('Mar 2017', 289), ('Apr 2017', 260), ('May 2017', 392), ('Jan 2018', 219), ('Feb 2018', 188), ('Mar 2018', 198), ('Apr 2018', 206)]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wanted_months = ['Jan 2017', 'Feb 2017', 'Mar 2017', 'Apr 2017', 'May 2017', 'Jan 2018', 'Feb 2018', 'Mar 2018', \n",
    "                'Apr 2018'] # In the 2017 data there were also comments from June to December\n",
    "\n",
    "nyt_comments_per_month = OrderedDict([('Jan 2017', 0), ('Feb 2017', 0), ('Mar 2017', 0), ('Apr 2017', 0), \n",
    "                                      ('May 2017', 0), ('Jan 2018', 0), ('Feb 2018', 0), ('Mar 2018', 0), \n",
    "                                      ('Apr 2018', 0)])\n",
    "\n",
    "staff_comments_per_month = OrderedDict([('Jan 2017', 0), ('Feb 2017', 0), ('Mar 2017', 0), ('Apr 2017', 0), \n",
    "                                      ('May 2017', 0), ('Jan 2018', 0), ('Feb 2018', 0), ('Mar 2018', 0), \n",
    "                                      ('Apr 2018', 0)])\n",
    "\n",
    "ethics_comments_per_month = OrderedDict([('Jan 2017', 0), ('Feb 2017', 0), ('Mar 2017', 0), ('Apr 2017', 0), \n",
    "                                      ('May 2017', 0), ('Jan 2018', 0), ('Feb 2018', 0), ('Mar 2018', 0), \n",
    "                                      ('Apr 2018', 0)])\n",
    "\n",
    "\n",
    "comments = 0\n",
    "\n",
    "comments_about_nyt_counter = 0\n",
    "comments_about_staff_counter = 0\n",
    "comments_about_ethics_counter = 0\n",
    "comments_in_both_categories = 0\n",
    "\n",
    "for file in files:\n",
    "    for entry in csv.DictReader( open( file, encoding='utf-8') ):\n",
    "        \n",
    "        comments += 1\n",
    "        \n",
    "        comment = entry['commentBody']\n",
    "        create_date = entry['createDate']\n",
    "        \n",
    "        stemmed_comment= ''\n",
    "        \n",
    "        comment_time = float(create_date)\n",
    "        \n",
    "        for keyword_nyt in keywords_nyt:\n",
    "            \n",
    "            was_in_staff = False\n",
    "            \n",
    "            if keyword_nyt in comment.lower():\n",
    "                \n",
    "                ## Stemming will only be done to comments that have one of the nyt-keywords since\n",
    "                ## stemming all the comments seems to take too long\n",
    "                \n",
    "                month_of_comment = datetime.datetime.fromtimestamp(int(comment_time)).strftime('%b %Y')\n",
    "                \n",
    "                if month_of_comment in wanted_months:\n",
    "                    \n",
    "                    nyt_comments_per_month[month_of_comment] = nyt_comments_per_month[month_of_comment] + 1\n",
    "                    \n",
    "                    for word in nltk.word_tokenize( comment.lower() ):\n",
    "                        stemmed_comment += stemmer.stem( word ) + ' '\n",
    "\n",
    "                    comments_about_nyt_counter += 1\n",
    "\n",
    "                    for keyword_staff in keywords_staff:\n",
    "                        if keyword_staff in stemmed_comment:\n",
    "                            comments_about_staff_counter += 1    \n",
    "                            staff_comments_per_month[month_of_comment] = staff_comments_per_month[month_of_comment] + 1\n",
    "                            was_in_staff = True\n",
    "                            break\n",
    "\n",
    "                    for keyword_ethics in keywords_ethics:\n",
    "                        if keyword_ethics in stemmed_comment:\n",
    "                            comments_about_ethics_counter += 1\n",
    "                            ethics_comments_per_month[month_of_comment] = ethics_comments_per_month[month_of_comment] + 1\n",
    "                            if was_in_staff == True:\n",
    "                                comments_in_both_categories += 1\n",
    "                            break\n",
    "\n",
    "                    break\n",
    "\n",
    "print( comments_about_nyt_counter, \"comments mention any of these:\", ','.join(keywords_nyt) )\n",
    "print( comments_about_staff_counter, \"comments mention any of these:\", ','.join(keywords_staff) )\n",
    "print( comments_about_ethics_counter, \"comments mention any of these:\", ','.join(keywords_ethics) )\n",
    "print( comments_in_both_categories, \"comments that mention staff and ethics keywords\")\n",
    "print( \"There are in total\", comments, 'comments\\n' )\n",
    "\n",
    "print('About nyt: ', nyt_comments_per_month, '\\n')\n",
    "print('About staff: ', staff_comments_per_month, '\\n')\n",
    "print('About ethics:', ethics_comments_per_month, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less comments were found than when the data was not stemmatized. In addition, without stemmatization we there were 5867 comments about staff and 2625 about ethics. With stemmatization we ended up having 2515 comments about staff and 2413 about journalistic guidelines. I would suggest that stemming the words makes the process more precise since stemming the words removes \"unnecessary part\". For example, in the stemmed ethics keywords \"diversity\" is translated to \"divers\". This, in turn, removes the need to have all the possible forms of the word (\"diversity\", \"diverse\" are both probably stemmed to \"divers\").  (However, for some reason I think that I shouls have found more comments after stemming..)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How would you use this to create a word-document matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To creating word-document matrix we should create, for example, a list of all the documents, or comments, that are about the staff. Then we would just use CountVectorizer to create the word-document matrix. For example:\n",
    "\n",
    "```\n",
    "vectorizing_tool = CountVectorizer()\n",
    "words_in_staff_documents = vectorizing_tool.transform(staff_documents)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some reflections\n",
    "\n",
    "The thing that became apparent when doing this exercise was that using dictionary methods is difficult. It is really difficult to find the right keywords that would be able to find all the relevant documents about the topic we are interested in. I guess that what is needed would be premilinary reading of some of the interesting documents (however, how we know which documents are  interesting if our goal is to find documents about some topic from a large body of documents?). In addition, we should have at least basic knowledge about the topic we are interested in and knowledge about the source of the data. Also, it would probably help to be a native speaker of the language in which the documents are. I found this exercise really difficult. \n",
    "\n",
    "Dictionary methods seem to be the most rudimentary method of \"data science\" in the sense that at least in its basic form it does not provide tools for analysis (e.g. an algorithm that provides an output that would simplify the data in some way). However, it is a method that is really easy to understand, it has clear parameters and does not appear as a black box to those who do not have the time resources to understand its mathematical background. This is because it does not have a mathematical background. Although I guess the only reason it could be considered as data science method is that computers makes it possible to go through a really large body of documents. My impression of dictionary methods is that it is a really useful method for finding wanted documents from a large body of documents which then can be then analyzed further with other methods. However, it is quite dependent on how good the researcher is in the \"craft of choosing the right keywords\". \n",
    "\n",
    "How I would utilize dictionary methods would be to first, find the interesting documents from a body of work. For example, all the documents that are about architecture from a large body of newspaper articles that are not indexed properly. After that this body of newspaper articles could be divided into those that are about buildings and those that are more about architecture. At least now I consider dictionary methods as something that can be used to build a solid foundation for the actual analysis that is done with some other machine learning method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
