{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create new data matrix for decision tree analysis\n",
    "import pandas as pd\n",
    "\n",
    "all_data = pd.read_csv( 'data/wvs.csv')\n",
    "\n",
    "data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables: \n",
    "\n",
    "* V10 - \"Feeling of happiness\" (1-2 happy, 3-4 not happy)\n",
    "* Important in life: V4 - family, V5 - friends, V6 - leisure time, V7 - politics, V8 - work, V9 - religion (1-2 important, 3-4 not important)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V10</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     V10     V4     V5     V6     V7     V8     V9\n",
       "0  False  False  False  False  False  False  False\n",
       "1  False  False  False   True   True  False  False\n",
       "2  False  False   True  False   True  False  False\n",
       "3  False  False  False   True   True   True  False\n",
       "4  False  False  False  False  False  False  False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = [ 'V10', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9']\n",
    "\n",
    "for variable in variables:\n",
    "    \n",
    "    data[ variable ] = all_data[ variable ] >= 3 ## 3-4 not happy / not important TRUE\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules = apriori( data, min_support=0.0000001, use_colnames = True )\n",
    "rules['length'] = rules['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "rules = association_rules( rules )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'V5', 'V6', 'V8'}) => frozenset({'V7'}) confidence 0.80369 lift 1.47483 support 0.00974\n",
      "frozenset({'V4', 'V10', 'V6', 'V8'}) => frozenset({'V7'}) confidence 0.83908 lift 1.53978 support 0.00082\n",
      "frozenset({'V5', 'V9', 'V10', 'V8'}) => frozenset({'V7'}) confidence 0.81641 lift 1.49817 support 0.00233\n",
      "frozenset({'V8', 'V4', 'V6', 'V7'}) => frozenset({'V5'}) confidence 0.80153 lift 6.61343 support 0.00117\n",
      "frozenset({'V5', 'V4', 'V6', 'V8'}) => frozenset({'V7'}) confidence 0.83333 lift 1.52924 support 0.00117\n",
      "frozenset({'V9', 'V4', 'V6', 'V8'}) => frozenset({'V5'}) confidence 0.81356 lift 6.71271 support 0.00107\n",
      "frozenset({'V9', 'V4', 'V6', 'V8'}) => frozenset({'V7'}) confidence 0.80508 lift 1.47740 support 0.00106\n",
      "frozenset({'V5', 'V9', 'V6', 'V8'}) => frozenset({'V7'}) confidence 0.81463 lift 1.49492 support 0.00373\n",
      "frozenset({'V6', 'V8', 'V5', 'V4', 'V10'}) => frozenset({'V7'}) confidence 0.87692 lift 1.60923 support 0.00064\n",
      "frozenset({'V6', 'V9', 'V5', 'V4', 'V10'}) => frozenset({'V7'}) confidence 0.84615 lift 1.55276 support 0.00061\n",
      "frozenset({'V9', 'V8', 'V5', 'V4', 'V10'}) => frozenset({'V7'}) confidence 0.80952 lift 1.48555 support 0.00057\n",
      "frozenset({'V6', 'V9', 'V8', 'V4', 'V10'}) => frozenset({'V7'}) confidence 0.86885 lift 1.59442 support 0.00059\n",
      "frozenset({'V6', 'V9', 'V8', 'V5', 'V10'}) => frozenset({'V7'}) confidence 0.86364 lift 1.58485 support 0.00148\n",
      "frozenset({'V6', 'V9', 'V7', 'V8', 'V4'}) => frozenset({'V5'}) confidence 0.84211 lift 6.94824 support 0.00089\n",
      "frozenset({'V6', 'V9', 'V8', 'V5', 'V4'}) => frozenset({'V7'}) confidence 0.83333 lift 1.52924 support 0.00089\n",
      "frozenset({'V9', 'V7', 'V8', 'V5', 'V4'}) => frozenset({'V6'}) confidence 0.82474 lift 3.93753 support 0.00089\n",
      "frozenset({'V6', 'V9', 'V8', 'V5', 'V4', 'V10'}) => frozenset({'V7'}) confidence 0.91304 lift 1.67551 support 0.00047\n",
      "frozenset({'V9', 'V7', 'V8', 'V5', 'V4', 'V10'}) => frozenset({'V6'}) confidence 0.82353 lift 3.93174 support 0.00047\n"
     ]
    }
   ],
   "source": [
    "for rule in rules.to_dict('records'):    \n",
    "    print( rule['antecedents'], '=>', rule['consequents'], \n",
    "            'confidence', '{0:.5f}'.format(rule['confidence']), \n",
    "            'lift', '{0:.5f}'.format(rule['lift']), \n",
    "             'support', '{0:.5f}'.format(rule['support']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Analyse also lift and support. Do you find any rules which might be interesting for further investigation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lift:\n",
      "\n",
      "1 frozenset({'V8', 'V4', 'V6', 'V7'}) => frozenset({'V5'}) confidence 0.80153 lift 6.61343 support 0.00117\n",
      "2 frozenset({'V9', 'V4', 'V6', 'V8'}) => frozenset({'V5'}) confidence 0.81356 lift 6.71271 support 0.00107\n",
      "3 frozenset({'V6', 'V9', 'V7', 'V8', 'V4'}) => frozenset({'V5'}) confidence 0.84211 lift 6.94824 support 0.00089\n",
      "4 frozenset({'V9', 'V7', 'V8', 'V5', 'V4'}) => frozenset({'V6'}) confidence 0.82474 lift 3.93753 support 0.00089\n",
      "5 frozenset({'V9', 'V7', 'V8', 'V5', 'V4', 'V10'}) => frozenset({'V6'}) confidence 0.82353 lift 3.93174 support 0.00047\n",
      "\n",
      "Best support:\n",
      "\n",
      "1 frozenset({'V5', 'V6', 'V8'}) => frozenset({'V7'}) confidence 0.80369 lift 1.47483 support 0.00974\n",
      "2 frozenset({'V5', 'V9', 'V10', 'V8'}) => frozenset({'V7'}) confidence 0.81641 lift 1.49817 support 0.00233\n",
      "3 frozenset({'V5', 'V9', 'V6', 'V8'}) => frozenset({'V7'}) confidence 0.81463 lift 1.49492 support 0.00373\n"
     ]
    }
   ],
   "source": [
    "rule_number = 1\n",
    "\n",
    "print(\"Best lift:\\n\")\n",
    "\n",
    "for rule in rules.to_dict('records'):\n",
    "    if rule['lift'] >= 3:\n",
    "        print(rule_number, rule['antecedents'], '=>', rule['consequents'], \n",
    "                'confidence', '{0:.5f}'.format(rule['confidence']), \n",
    "                'lift', '{0:.5f}'.format(rule['lift']), \n",
    "                 'support', '{0:.5f}'.format(rule['support']) )\n",
    "        rule_number += 1\n",
    "\n",
    "rule_number = 1\n",
    "\n",
    "print(\"\\nBest support:\\n\")\n",
    "\n",
    "for rule in rules.to_dict('records'):\n",
    "    if rule['support'] >= .002:\n",
    "        print(rule_number, rule['antecedents'], '=>', rule['consequents'], \n",
    "                'confidence', '{0:.5f}'.format(rule['confidence']), \n",
    "                'lift', '{0:.5f}'.format(rule['lift']), \n",
    "                 'support', '{0:.5f}'.format(rule['support']) )\n",
    "        rule_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five rules that have a high lift. Which means, for example in the case of rule 2, that it is very likely that when the responder does NOT consider family (V4) AND religion (V9) AND leisure time (V6) AND work (V8) to be important for them, it is very likely that they do NOT consider friends (V5) important. The other rules with high lift (>6) were also about friends. They seem to describe some kind of \"general apathy\" towards everything. Hence, there are individuals who do not think that anything is important for them. However, what is missing from these rules is interesting. Not being happy (V10) is present only in one of these rules. Which goes against \"folk sociological/psychological\" idea that this kind of general apathy toward everything should also be associated with not being happy. \n",
    "\n",
    "However, when we look at the rules that have the highest support, none of the rules with high lift belong to them. In the best cases of the rules with the best lift is only 0.001 which means that only 0.1% of the cases had this rule in them. Hence, I would not consider that the rules we found provide us interesting knowledge about how the variables are associated to each other in general. All the rules with the best support had quite small lift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try adding more variables. Note that these can only be True/False variables.\n",
    "\n",
    "The variables that are added are those that answer the questions of \"Maritial status\" (V57) and \"How many children do you have?\" (V58). The first reason for choosing these variables is that they are true/false variables. In addition, an interesting question is are these variables associated with what people consider as important or being happy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     V10     V4     V5     V6     V7     V8     V9    V57    V58\n",
      "0  False  False  False  False  False  False  False   True   True\n",
      "1  False  False  False   True   True  False  False   True   True\n",
      "2  False  False   True  False   True  False  False   True   True\n",
      "3  False  False  False   True   True   True  False   True   True\n",
      "4  False  False  False  False  False  False  False  False  False\n"
     ]
    }
   ],
   "source": [
    "data_with_extra_variables = pd.DataFrame()\n",
    "\n",
    "variables = [ 'V10', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V57', 'V58' ]\n",
    "\n",
    "for variable in variables:\n",
    "    if variable == 'V58':\n",
    "        data_with_extra_variables[ variable ] = all_data[ variable ] < 1 ## no children TRUE\n",
    "    else:\n",
    "        data_with_extra_variables[ variable ] = all_data[ variable ] >= 3 ## 3-4 not happy / not important  TRUE\n",
    "                                                                          ## 3-6 not married TRUE\n",
    "\n",
    "print(data_with_extra_variables.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = apriori( data_with_extra_variables, min_support=0.001, use_colnames = True )\n",
    "rules['length'] = rules['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "rules = association_rules( rules )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lift >= 2.3: \n",
      "\n",
      "1 frozenset({'V9', 'V4', 'V58'}) => frozenset({'V57'}) confidence 0.86364 lift 2.32371 support 0.00297\n",
      "2 frozenset({'V4', 'V10', 'V7', 'V58'}) => frozenset({'V57'}) confidence 0.85714 lift 2.30624 support 0.00121\n",
      "3 frozenset({'V8', 'V4', 'V6', 'V7'}) => frozenset({'V5'}) confidence 0.80153 lift 6.61343 support 0.00117\n",
      "4 frozenset({'V9', 'V4', 'V6', 'V8'}) => frozenset({'V5'}) confidence 0.81356 lift 6.71271 support 0.00107\n",
      "5 frozenset({'V58', 'V4', 'V6', 'V7'}) => frozenset({'V57'}) confidence 0.86842 lift 2.33658 support 0.00111\n",
      "6 frozenset({'V9', 'V4', 'V58', 'V8'}) => frozenset({'V57'}) confidence 0.86667 lift 2.33186 support 0.00116\n",
      "\n",
      "Support >= 0.01: \n",
      "\n",
      "1 frozenset({'V10', 'V7', 'V58'}) => frozenset({'V57'}) confidence 0.81648 lift 2.19684 support 0.02245\n",
      "2 frozenset({'V9', 'V10', 'V58'}) => frozenset({'V57'}) confidence 0.80609 lift 2.16887 support 0.01123\n"
     ]
    }
   ],
   "source": [
    "rule_count = 1\n",
    "\n",
    "print('Lift >= 2.3: \\n' )\n",
    "for rule in rules.to_dict('records'):\n",
    "    if rule['lift'] >= 2.3:\n",
    "        print(rule_count, rule['antecedents'], '=>', rule['consequents'], \n",
    "                'confidence', '{0:.5f}'.format(rule['confidence']), \n",
    "                'lift', '{0:.5f}'.format(rule['lift']), \n",
    "                 'support', '{0:.5f}'.format(rule['support']) )\n",
    "        rule_count += 1\n",
    "\n",
    "rule_count = 1\n",
    "\n",
    "print('\\nSupport >= 0.01: \\n' )\n",
    "for rule in rules.to_dict('records'):\n",
    "    if rule['support'] >= 0.01:\n",
    "        print(rule_count, rule['antecedents'], '=>', rule['consequents'], \n",
    "                'confidence', '{0:.5f}'.format(rule['confidence']), \n",
    "                'lift', '{0:.5f}'.format(rule['lift']), \n",
    "                 'support', '{0:.5f}'.format(rule['support']) )\n",
    "        rule_count += 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With added variables, the some of the rules that had a high lift in the previous section (e.g. V8, V4, V6, V7 -> V5) had the best lift. However, some rules with higher support (>0.01) emerged. One of these rules suggest that when a person is not happy AND does not consider politics important AND is not married they probably do not have any children. Other rule is similar in other but instead of NOT considering politics important they do NOT consider religion important. It may not be surprising that not being married and not having children are associated. However, I was surprised how there were no significant associations between what person considers to be important and are they not happy. However, there seems to be at least some kind of association between not being married and unhappiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some reflections\n",
    "\n",
    "For some reason I did not find association rules method as an interesting machine learning method. Perhaps it is the lack of my imagination but I was not able to think of that many (realistic) reserach questions that would require the using of association rules. This of course can be caused by two facors. The class reading which was about leaders and government systems in different countries did not provide any kind of revelation of how powerful and useful this method could be when \"used the right way\". Also, the frequent \"shopping basket\" example of which is used to explain what association rules methods are about (obviously I had to google about association rules a lot) did frame association rules in a way that I kept me wondering \"what kind of social scientific questions are similar to this task\". \n",
    "\n",
    "I guess that I can think of at least one case where association rules analysis would be provide an interesting possibility (although it is quite unrealistic case). Rogers Brubaker and his research crew did a study about how \"ethnicity works\" (the study is reported in their book _Nationalistic Politics and Everyday Ethnicity in an Transsylvanian Town_). One interesting research stream was the question of when and how some interaction situation and those who are participating it becomes framed as \"ethnic\". They collected data with ethnography and interviews. Analyzing this data produced some kind of general understanding of what kind of cues activated ethnic schemas -- for example, a name that connected a stranger to the Hungarian minority was this kind of cue. It could be claimed that they produced some very general association rules between cues and the activation of ethnic schemas. In theory, this data (or similar large data) could be processed (or collected) in a way that there would be TRUE/FALSE statements of what was present in the interaction situation where ethnicity did nor did not emerge. If this data was large enough, it could be analysed with association rules method and it could provide interesting and more detailed rules about how environment, different cues and the activation of different ethnic schemas are connected to each other. This could help to illustrate some aspects of the interaction between cognition, action and the environment. \n",
    "\n",
    "What I learned from this exercise was that, at least with the data that was used (and the variables I chose), it was really difficult to find rules that could be considered to be interesting (in substance and also be statistically interesting). The reason could be that this kind of data just does not provide a good foundation for doing association rules analysis. However, although I am a bit sceptical towards this method, it could be that with the right data and research question this method could be used to find interesting things about the social world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
