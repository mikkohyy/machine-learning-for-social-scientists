{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic models\n",
    "\n",
    "## Dataset for the exercise\n",
    "\n",
    "* [New York Times Comments](https://www.kaggle.com/aashita/nyt-comments/data), set of reders' comments for articles published in the New York Times\n",
    "\n",
    "## Overarching research question\n",
    "\n",
    "The comments allow a perspective to study what kind of concerns people raise when commenting to online articles.\n",
    "Examine if meaninful themes emerge from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sample size 5032\n"
     ]
    }
   ],
   "source": [
    "## data collection from files.\n",
    "## to keep the dataset fairly small, we conduct random data selection here.\n",
    "## this is *ONLY* to ensure that the model is suitable for teaching purposes\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "path = 'data/nyt/'\n",
    "files = os.listdir( path ) ## see all files in directory\n",
    "files = filter( lambda file_name: file_name.startswith(\"CommentsApril\"), files ) ## choose only data files for April\n",
    "files = map( lambda file_name: path + file_name, files ) ## add path to file names\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file in files:\n",
    "    for entry in csv.DictReader( open( file, encoding='utf-8' ) ):\n",
    "        \n",
    "        if random.random() > .99: ## choose content randomly\n",
    "            comment = entry['commentBody']\n",
    "\n",
    "            documents.append( comment )\n",
    "            \n",
    "print(\"Data sample size\", len(documents) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From text data to document-term matrix\n",
    "\n",
    "To analyse textual data we transform it to a document term matrix, where in rows we have documents (different comments) and columns represent each word in the dataset.\n",
    "\n",
    "Note how we **preprocess** the text during this quantification. We remove stopwords (through a set of common English stopwords; we could also create our own lists), stem the content of comments to ensure language is treated well and lower case everything in the content. Thus, the `document_terms` is a huge sparse matrix in the end. Preprocessing is its own kind of art, as it can [influence results](https://www.cambridge.org/core/product/identifier/S1047198717000444/type/journal_article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For some reason the original code did not remove the stop words. Hence, I had to modify the code.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "added_stop_words = text.ENGLISH_STOP_WORDS.union(['http', 'www', 'https', 'html', 'href', \n",
    "                                                 'target', 'title', 'blank']) # Adding these words to stop words since \n",
    "                                                                                   # they appeared in the topics but seemed \n",
    "                                                                                   # to be something that did not have\n",
    "                                                                                   # any meaning\n",
    "            \n",
    "stemmer = EnglishStemmer()\n",
    "regex_tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "processed_documents = []\n",
    "\n",
    "for document in documents:\n",
    "    stemmed_document = ''\n",
    "    \n",
    "    tokenized_document = regex_tokenizer.tokenize(document)\n",
    "    \n",
    "    for word in tokenized_document:\n",
    "        if len(word) >= 4:\n",
    "            stemmed_document += stemmer.stem(word) + ' '\n",
    "    \n",
    "    processed_documents.append(stemmed_document)\n",
    "    \n",
    "tf_vectorizer = CountVectorizer(max_df=0.90, min_df=10, stop_words=added_stop_words, lowercase = True)\n",
    "\n",
    "document_terms = tf_vectorizer.fit_transform(processed_documents)\n",
    "document_terms_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From document-term matrix to analysis\n",
    "\n",
    "Finally we run the Latent Dirichlet Allocation process to our matrix to create topics.\n",
    "Similar to k-means, we choose the number of topics; there are also other parameters which could be used to _fine tune_ topic models, see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) for details.\n",
    "When I use these we with their default parameters as none of them solves the challenge that [topic models work on a different abstration level than humans](http://doi.wiley.com/10.1002/asi.23786)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_five = LatentDirichletAllocation( n_components = 5 )\n",
    "model_five = lda_five.fit( document_terms )\n",
    "\n",
    "lda_ten = LatentDirichletAllocation( n_components = 10 )\n",
    "model_ten = lda_ten.fit( document_terms )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "like | peopl | time | just | think | \n",
      "Topic 2\n",
      "peopl | mani | live | onli | need | \n",
      "Topic 3\n",
      "trump | presid | elect | vote | republican | \n",
      "Topic 4\n",
      "work | peopl | school | year | need | \n",
      "Topic 5\n",
      "right | peopl | make | white | like | \n",
      "\n",
      "Topic 1\n",
      "trump | republican | presid | vote | democrat | \n",
      "Topic 2\n",
      "peopl | comey | person | white | news | \n",
      "Topic 3\n",
      "state | trump | korea | north | countri | \n",
      "Topic 4\n",
      "peopl | make | mani | care | right | \n",
      "Topic 5\n",
      "trump | like | presid | know | believ | \n",
      "Topic 6\n",
      "year | love | time | work | alway | \n",
      "Topic 7\n",
      "just | think | watch | word | right | \n",
      "Topic 8\n",
      "nytim | target | titl | blank | href | \n",
      "Topic 9\n",
      "russia | syria | trump | weapon | assad | \n",
      "Topic 10\n",
      "peopl | school | work | like | year | \n"
     ]
    }
   ],
   "source": [
    "for topic_number, words in enumerate( model_five.components_ ):\n",
    "        print( \"Topic\", topic_number+1 )\n",
    "        for word in words.argsort()[:-6:-1]:\n",
    "            print( document_terms_names[word], end=' | ' )\n",
    "        print()\n",
    "\n",
    "print()\n",
    "        \n",
    "for topic_number, words in enumerate( model_ten.components_ ):\n",
    "        print( \"Topic\", topic_number+1 )\n",
    "        for word in words.argsort()[:-6:-1]:\n",
    "            print( document_terms_names[word], end=' | ' )\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34768398, 0.0066205 , 0.38051711, 0.17378746, 0.09139096]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see the distribution of a document to different topics\n",
    "model.transform( document_terms[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "* Compute the distribution of all documents to each topic. Where could you use this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Topic 1': 1256, 'Topic 2': 473, 'Topic 3': 839, 'Topic 4': 1444, 'Topic 5': 1020} \n",
      "\n",
      "{'Topic 1': 0.23836363966915844, 'Topic 2': 0.11719937345645313, 'Topic 3': 0.17815369397478192, 'Topic 4': 0.2616059700449289, 'Topic 5': 0.20467732285467805}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_of_topics = len(model.transform(document_terms[0])[0])\n",
    "\n",
    "documents_on_topic = {}\n",
    "\n",
    "## First let's see the number of documents for each topic (meaning that if Topic 1 is prevelent in a document\n",
    "## it is classified to belong to the class of Topic 1)\n",
    "\n",
    "for i in range(0, n_of_topics):\n",
    "    key = 'Topic {}'.format(i+1)\n",
    "    documents_on_topic[key] = 0\n",
    "    \n",
    "for line in model.transform(document_terms):\n",
    "    index_of_max = np.where(line == np.max(line))\n",
    "    index_of_max = index_of_max[0][0]\n",
    "    add_to_key = 'Topic {}'.format(index_of_max+1)\n",
    "    documents_on_topic[add_to_key] += 1\n",
    "    \n",
    "print(documents_on_topic, '\\n')\n",
    "\n",
    "## Then how the topics are distributed in all the documents:\n",
    "\n",
    "freq_on_topic = {}\n",
    "n_of_topics = len(model.transform(document_terms[0])[0])\n",
    "\n",
    "n_of_documents = len(model.transform(document_terms))\n",
    "\n",
    "for i in range(0, n_of_topics):\n",
    "    key = 'Topic {}'.format(i+1)\n",
    "    freq_on_topic[key] = 0\n",
    "\n",
    "for line in model.transform(document_terms):\n",
    "    for i in range(0, n_of_topics):\n",
    "        add_this = line[i]\n",
    "        to_key = add_to_key = 'Topic {}'.format(i+1)\n",
    "        freq_on_topic[to_key] += add_this\n",
    "        \n",
    "freq_on_topic.update((key, value / n_of_documents) for key, value in freq_on_topic.items())\n",
    "\n",
    "print(freq_on_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the understanding of which topic is the most common topic in the data. This would mean that we can, for example, get some kind of understanding of what was the most discussed topic at a certain point of time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify the code and examine a few potential topic numbers. What differences can you detect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the topics were relatively difficult to interpret, with ten topics there emerged some kind of understanding of what the topics could be about (although they are still quite messy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modify the preprocessing and remove all words which are shorter than four characters. What do you learn now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the words that are shorter than four characters makes the topics more clearly defined and it is possible to find much clearly defined topics. For example, with five topics there are three topics about Trump, something about school and something about money. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "There are many different approaches to evaluate topic models (see, [1](http://doi.acm.org/10.1145/1553374.1553515), [2](https://journal.fi/politiikka/article/view/79629) for examples).\n",
    "We can evaluate the suitability of topic models using statistical measurements like loglikelihood, but [some say](http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf) that this might be a bad practice - and [others](https://journal.fi/politiikka/article/view/79629) recommend it.\n",
    "Here we show how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-947498.2873321398"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score( document_terms )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "With five topics the score was -949756.89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "\n",
    "* Evaluate a set of different topics based on this score. Which one would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with different number of topics:\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "topic_n = [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 16, 17, 18, 19, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "for i in topic_n:\n",
    "    lda = LatentDirichletAllocation( n_components = i )\n",
    "    model = lda.fit( document_terms )\n",
    "    model_results[i] = model.score( document_terms )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, -935891.0939410102), (2, -938158.3617629377), (4, -938746.3039306123), (5, -943997.9996749549), (6, -945350.2084943934), (7, -946434.3938784814), (8, -949231.9917409667), (9, -950210.0164142104), (10, -950772.5202008706), (16, -960606.8646228923)]\n"
     ]
    }
   ],
   "source": [
    "# The ten best number of topics with their log-likelihoods: \n",
    "\n",
    "print(sorted(model_results.items(), key=lambda x: x[1], reverse=True)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there does not seem to be that much difference between the models. They just seem to be getting gradually worse when more topics are added. Interestingly a model with 16 topics is the 10th best model according to log-likelihood. However, if we have to choose some of these models, I do not think that these results are that helpful. What follows is an attempt to evaluate different number of topics manually. I will compare the models with the numbers of topics 2, 3, 4, 5, 7, 10 and 16. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics: 2\n",
      "Topic 1\n",
      "peopl | work | like | year | just | time | mani | think | need | make | \n",
      "Topic 2\n",
      "trump | presid | like | peopl | republican | vote | elect | onli | just | democrat | \n",
      "\n",
      "Topics: 3\n",
      "Topic 1\n",
      "work | year | like | peopl | just | time | make | becaus | live | need | \n",
      "Topic 2\n",
      "trump | presid | like | vote | elect | republican | peopl | just | democrat | know | \n",
      "Topic 3\n",
      "peopl | like | think | right | state | mani | make | world | want | just | \n",
      "\n",
      "Topics: 4\n",
      "Topic 1\n",
      "peopl | need | make | work | money | care | becaus | year | mani | like | \n",
      "Topic 2\n",
      "like | time | just | nytim | world | read | love | thing | year | think | \n",
      "Topic 3\n",
      "trump | presid | republican | vote | elect | democrat | like | parti | peopl | polit | \n",
      "Topic 4\n",
      "trump | peopl | work | just | year | like | know | onli | think | time | \n",
      "\n",
      "Topics: 5\n",
      "Topic 1\n",
      "trump | presid | republican | like | elect | vote | democrat | peopl | just | support | \n",
      "Topic 2\n",
      "peopl | work | like | time | year | just | mani | need | becaus | women | \n",
      "Topic 3\n",
      "school | educ | like | public | think | student | year | work | hope | privat | \n",
      "Topic 4\n",
      "state | money | peopl | govern | countri | like | just | tax | right | militari | \n",
      "Topic 5\n",
      "peopl | time | make | white | just | fact | becaus | chang | onli | world | \n",
      "\n",
      "Topics: 7\n",
      "Topic 1\n",
      "trump | democrat | republican | like | presid | right | polit | parti | vote | elect | \n",
      "Topic 2\n",
      "need | peopl | like | life | work | just | school | women | children | becaus | \n",
      "Topic 3\n",
      "trump | presid | russia | syria | elect | russian | korea | militari | attack | time | \n",
      "Topic 4\n",
      "peopl | work | just | like | time | mani | famili | veri | onli | live | \n",
      "Topic 5\n",
      "nytim | thank | time | read | love | titl | world | year | watch | hope | \n",
      "Topic 6\n",
      "state | year | govern | money | peopl | cost | need | unit | care | make | \n",
      "Topic 7\n",
      "peopl | think | trump | like | becaus | white | thing | just | onli | american | \n",
      "\n",
      "Topics: 10\n",
      "Topic 1\n",
      "trump | republican | democrat | vote | parti | elect | voter | peopl | presid | state | \n",
      "Topic 2\n",
      "nytim | titl | think | world | trade | data | china | want | collect | person | \n",
      "Topic 3\n",
      "like | peopl | time | work | know | comey | mani | women | year | make | \n",
      "Topic 4\n",
      "state | peopl | public | just | want | educ | right | need | school | class | \n",
      "Topic 5\n",
      "trump | presid | russia | obama | syria | russian | attack | like | elect | assad | \n",
      "Topic 6\n",
      "korea | north | health | care | peopl | right | make | good | china | south | \n",
      "Topic 7\n",
      "year | veri | peopl | trump | world | school | american | like | becaus | great | \n",
      "Topic 8\n",
      "peopl | live | love | human | like | life | feel | comment | person | just | \n",
      "Topic 9\n",
      "time | like | mani | just | school | year | read | airlin | unit | high | \n",
      "Topic 10\n",
      "trump | peopl | money | just | like | work | onli | make | year | countri | \n",
      "\n",
      "Topics: 16\n",
      "Topic 1\n",
      "korea | north | year | state | make | trump | onli | presid | south | million | \n",
      "Topic 2\n",
      "american | peopl | like | white | onli | countri | chang | hate | live | need | \n",
      "Topic 3\n",
      "care | health | peopl | time | insur | becaus | make | just | like | work | \n",
      "Topic 4\n",
      "trump | news | presid | think | good | onli | countri | like | time | make | \n",
      "Topic 5\n",
      "weapon | peopl | work | just | chemic | use | syria | assad | kill | trump | \n",
      "Topic 6\n",
      "school | educ | work | colleg | student | year | public | teacher | high | realli | \n",
      "Topic 7\n",
      "vote | democrat | elect | like | peopl | republican | parti | voter | just | berni | \n",
      "Topic 8\n",
      "trump | presid | russia | russian | comey | investig | elect | mueller | putin | obama | \n",
      "Topic 9\n",
      "money | peopl | like | busi | just | compani | govern | good | need | right | \n",
      "Topic 10\n",
      "nytim | world | women | titl | like | collect | love | girl | action | parent | \n",
      "Topic 11\n",
      "trump | vote | white | peopl | black | ryan | onli | think | american | women | \n",
      "Topic 12\n",
      "polit | right | liber | think | like | peopl | conserv | believ | base | just | \n",
      "Topic 13\n",
      "peopl | know | life | alway | mani | human | think | death | just | becaus | \n",
      "Topic 14\n",
      "need | peopl | mani | articl | famili | year | make | time | live | doctor | \n",
      "Topic 15\n",
      "trump | republican | senat | democrat | presid | like | year | parti | time | unit | \n",
      "Topic 16\n",
      "govern | tax | mani | want | republican | peopl | state | cut | econom | public | \n"
     ]
    }
   ],
   "source": [
    "topic_n = [2, 3, 4, 5, 7, 10, 16]\n",
    "\n",
    "for i in topic_n:\n",
    "    print('\\nTopics:', i)\n",
    "    lda = LatentDirichletAllocation( n_components = i )\n",
    "    model = lda.fit( document_terms )\n",
    "    for topic_number, words in enumerate( model.components_ ):\n",
    "        print( \"Topic\", topic_number+1 )\n",
    "        for word in words.argsort()[:-11:-1]:\n",
    "            print( document_terms_names[word], end=' | ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least for me, it was quite difficult to make much sens of most of these topics (probably more of some kind of \"domain knowledge\" would have helped in their interpretation). However, it seems that Trump pops out in many of the topics. Hence, we can say with a confidence that Trump was a central topic in the comments in the NYT website. Of these models, the easiest to interpret was the one that had seven topics. According to it the concerns raised by the commentators could be interpreted as follows: \n",
    "\n",
    "Topic 1\n",
    "trump | democrat | republican | like | presid | right | polit | parti | vote | elect | \n",
    "\n",
    "* Topic 1: Trump and elections\n",
    "\n",
    "Topic 2\n",
    "need | peopl | like | life | work | just | school | women | children | becaus | \n",
    "\n",
    "* Topic 2: Probably about some kind of work/life/family balance especially with women\n",
    "\n",
    "Topic 3\n",
    "trump | presid | russia | syria | elect | russian | korea | militari | attack | time | \n",
    "\n",
    "* Topic 3: Trump and foreign policy\n",
    "\n",
    "Topic 4\n",
    "peopl | work | just | like | time | mani | famili | veri | onli | live | \n",
    "\n",
    "* Topic 4: Probably also about some kind of work/life/family balance \n",
    "\n",
    "Topic 5\n",
    "nytim | thank | time | read | love | titl | world | year | watch | hope | \n",
    "\n",
    "* Topic 5: About New York Times the magazine\n",
    "\n",
    "Topic 6\n",
    "state | year | govern | money | peopl | cost | need | unit | care | make | \n",
    "\n",
    "* Topic 6: Could be about state, money and healthcare\n",
    "\n",
    "Topic 7\n",
    "peopl | think | trump | like | becaus | white | thing | just | onli | american | \n",
    "\n",
    "* Topic 7: Could be about alt-right, trump and the internet\n",
    "\n",
    "It is quite probable that in media there were lots of discussion on Trump (as it has been for the last 3,5 years). I am not sure if the other topics (work/life/family, magazine or state, monay and healthcare) were somehow relevant when the data was created. Other aspect of the data used, that also makes the interpretation difficult, is that it contains comments from April 2017 and also from April 2018. Hence comments that are an year apart from each other are treated as similar. This would not be a thing worth mentioning if we had a larger data, but when we have (only part of the) comments from two months, it can influence the topic modeling in a bad way. The discussion on \"work/life/family\" balance could emerge during that time of the years since the summer holidays are closing in and there _could_ be articles that discuss these themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some reflections\n",
    "\n",
    "Topic modeling was  method that I was most exited about since was the method that was most clearly associated with something that is familiar to me, that is, analysing texts. However, similarily to other exercises that were done with textual data, this exercise once illustrated how difficult it is to use textual data and how the preprocessing is some kind of handcraft (or magic) that has a lot of tacit knowledge involved. In addition, topic models raised the question of how well should we know the data we using to build the topic model. It is quite obvious that we should know something about the context (e.g. when, where and how it was created). However, how much should we know about the data itself. If we know the data too well, we probably end up selecting the number of topics that mirrors our own classification scheme (which is probably based on relatively small sample of the data). This then sidelines one of the interesting aspects of topic modeling, that it can produce surprising results (or at least I think that this is one interesting possibility). I think that there is some kind of tension between \"knowing the data you use\" and \"giving the algorithm a chance to give new perspective to the textual data\". \n",
    "\n",
    "Obviously, the hardest part in this exercise was to interpret the topics that the algorithm found (in my case the model with seven topics was the most easiest to interpret). This aspect of the topic modeling also quite difficult (especially since I do not have that much understanding of what we should expect from the comments section of NYT). For example, I kept wondering how many words we should use to define a topic. The original code showed only five words. I modified it to show ten words (since with five words it was almost impossible to interpret the topics). However, a question arised about \"how many words we should use to define a topic\". Should we just look for more and more words that characterise the topic or should we just give up at some point and admit that perhaps we found topics that are not actually that easy to interpret. I wondered if there were any rules of thumb for this. \n",
    "\n",
    "The exercise also raised a question about the relationship between researcher and the tool (topic modeling). Should we think about the tool as something that is forced to follow our intuition (i.e. if we gain a hunch that some kind of topics could arise from the data, and we are kind of allowed to follow this intuition and process the data to remove the \"noise\" from the data to make the topics we found much clearer and clearer) or should we treat topic modeling as a tool that summarises the data in a way that we could not ever been able to do. Hence, is it a tool for clarifying our own ideas or a tool which guidance we should just follow. \n",
    "\n",
    "Similarily to support vector machines, this exercise raised questions about the computational resources needed to run topic modeling if we did real research. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
